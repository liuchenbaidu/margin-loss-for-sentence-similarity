# margin-loss-for-sentence-similarity
将人脸识别领域的margin loss等技术用于句子相似度匹配(构建同义句组)

工作正在进行中，尚不完善，仅做代码备份和想法交流使用。

### 数据(网上目前尚未找到类似数据，该数据暂时不准备开源，有合作意向可以分享)

构建约30W组同义句组，从新浪爱问进行爬取。每组同义句的数量在6到17之间。考虑了翻译，相似性传递，人工规则构造等方式，最后还是选择了从网络上爬取。

NLP领域传统的解决句子相似度的方法针对的语料可以认为是每组同义句组的数量为2。因此，该数据的特色主要体现在两个方面：规模较大和同义单句数量>2。

### 人脸识别损失函数

度量学习的一些思想可以引入人脸识别，同样人脸识别的技术或许可以用于NLP的句子相似度匹配。

### 想法延伸

基于该数据或许可以有新的想法，欢迎讨论。

### 代码

PyTorch版本=1.0.0，实现了人脸中的一个margin loss(am softmax loss)，类似loss有很多，可以具体参照CV领域的相关文章。

### 类似工作

[基于GRU和am-softmax的句子相似度模型](https://spaces.ac.cn/archives/5743/comment-page-1)，该工作给出了Keras版的实现。从另外一个角度来看，文章做的工作可能还不够深入，可以继续深入讨论。
